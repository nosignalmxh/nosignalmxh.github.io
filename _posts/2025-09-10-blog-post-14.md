---
title: 'Flow Matching 深度解析：从理论到实践的完整指南'
date: 2025-09-10
permalink: /posts/2025/09/blog-post-14/
tags:
  - 教程
  - 机器学习
  - 生成模型
  - 数学
---

在学习了[**MIT 6.S184**](https://arxiv.org/abs/2506.02070)关于利用SDE和ODE理论推导diffusion model和flow模型的课程后，我对flow模型产生了浓厚兴趣。虽然在单细胞分析、虚拟细胞建模、空间转录组学等领域已有许多采用flow matching的研究论文，但仅仅阅读这些论文难以获得对flow matching的体系化认识。因此，我决定深入学习[Flow Matching Guide and Code](https://arxiv.org/abs/2412.06264)这篇综述，以期对flow matching产生系统性的理解。这篇博客主要是介绍了前四章的内容，以求对于fm有一个全面的了解。

本文将从生成模型的基础概念出发，逐步深入探讨flow matching的核心理论、数学推导、实现细节以及实际应用，旨在为读者提供一个完整的flow matching学习路径。

---

# 第一章 生成模型的本质与Flow模型的直觉

## 1.1 生成模型的核心思想

生成模型的本质是通过学习大量真实数据来掌握数据集的分布规律，进而能够创造出世界上不存在但极其逼真的同类型数据。这个过程本质上就是**学习数据集的概率分布**。

## 1.2 Flow模型的基本原理

由于目标分布往往极其复杂，几乎所有的生成模型都采用相同的策略：

1. 从一个**简单的已知分布**（通常是高斯分布）出发
2. 通过一个**复杂的可学习变换**，逐步逼近真实数据的分布

Flow模型的独特之处在于：它通过学习一个**速度场**（也称为向量场）来指导每个噪声点如何在概率空间中移动，最终到达目标分布的正确位置。可以将其想象为粒子在向量场的引导下，沿着最优路径从起点流向终点。

---

# 第二章 Flow Matching的核心概念与公式体系

## 2.1 Flow Matching的基本目标

Flow matching要解决的核心问题是：
- **输入**：一堆真实样本，它们服从复杂的目标分布 $q$
- **输出**：一个能够生成与原始分布一致的新样本的生成模型

## 2.2 概率路径的构造

Flow matching的核心思想是利用**概率路径**：
- **起点**：源分布 $p_0 = p$（通常为标准高斯分布）
- **终点**：目标分布 $p_1 = q$（真实数据分布）
- **路径**：中间每个时刻 $t$ 对应一个概率分布 $p_t$

## 2.3 线性插值路径

在实际训练中，flow matching采用最简单的线性插值策略：


$$X_t = (1-t)X_0 + tX_1$$

其中：
- $X_0$：从源分布 $p$ 中采样的噪声点
- $X_1$：从目标分布 $q$ 中采样的真实数据点
- $t \in [0,1]$：时间参数

## 2.4 速度场的计算

通过对线性插值路径求导，我们得到速度场的解析表达式：


$$\frac{d(X_t)}{dt} = \frac{d}{dt}((1-t)X_0 + tX_1) = -X_0 + X_1 = X_1 - X_0$$

这意味着粒子以恒定速度从起点移向终点。

## 2.5 条件概率路径与边缘化

**条件概率路径** $p_{t\mid 1}(x\mid x_1)$ 是针对特定目标点 $x_1$ 的路径。整体的概率路径通过边缘化得到：


$$p_t(x) = \int p_{t|1}(x|x_1)q(x_1)dx_1$$

## 2.6 损失函数的设计

**原始Flow Matching损失**：

$$L_{FM}(\theta) = E_{t, X_t} \|u_t^\theta(X_t) - u_t(X_t)\|^2$$

由于边缘速度场 $u_t(X_t)$ 难以计算，我们转向使用**条件速度场**：


$$u_t(x|x_1) = \frac{x_1 - x}{1-t}$$

**条件Flow Matching损失**：

$$L_{CFM}(\theta) = E_{t, X_0, X_1} \|u_t^\theta(X_t|X_1) - u_t(X_t|X_1)\|^2$$

重要的是，两种损失函数的梯度相等：$\nabla_\theta L_{FM}(\theta) = \nabla_\theta L_{CFM}(\theta)$

**实用的简化形式**：

$$L_{CFM}^{OT,Gauss}(\theta) = E_{t,X_0,X_1} \|u_t^\theta(X_t) - (X_1 - X_0)\|^2$$

---

# 第三章 Flow模型的数学基础

## 3.1 随机向量与概率密度函数

### 基本概念
- **随机向量(Random Vector)**：$X \in \mathbb{R}^d$ 表示在d维空间中的随机点
- **概率密度函数(PDF)**：$p(x)$ 描述随机向量在不同位置的出现概率

### 基本公式
**概率计算**：

$$P(X \in A) = \int_A p_X(x)dx$$

**期望值**：

$$E[X] = \int x \cdot p_X(x)dx$$

**无意识统计学家定律**：

$$E[f(X)] = \int f(x) \cdot p_X(x)dx$$

## 3.2 条件密度与期望

### 联合分布与边缘分布
- **联合密度**：$p_{X,Y}(x, y)$ 描述两个随机向量的同时分布
- **边缘密度**：$p_X(x) = \int p_{X,Y}(x, y)dy$

### 条件概率密度

$$p_{X|Y}(x|y) = \frac{p_{X,Y}(x, y)}{p_Y(y)}$$

### 贝叶斯法则

$$p_{Y|X}(y|x) = \frac{p_{X|Y}(x|y) \cdot p_Y(y)}{p_X(x)}$$

### 条件期望

$$E[X | Y = y] = \int x \cdot p_{X|Y}(x|y)dx$$

注意：$E[X \mid Y=y]$ 是关于 $y$ 的确定函数，而 $E[X \mid Y]$ 是随机变量。

### 全期望定律

$$E[E[X|Y]] = E[X]$$

**直观理解**：全国学生平均身高 = 各班级平均身高的平均值。

## 3.3 微分同胚与前推映射

### 核心概念
- **微分同胚**：光滑且可逆的变换
- **前推映射**：当我们变换概率空间时，概率分布也相应"被推动"

### 函数空间 $C^r(\mathbb{R}^m, \mathbb{R}^n)$
包含所有满足以下条件的函数：函数的每个输出分量关于输入变量的任意组合的r阶偏导数都存在且连续。

### 变量替换公式
对于变换 $Y = \psi(X)$：

$$p_Y(y) = p_X(\psi^{-1}(y)) \left|\det \frac{\partial \psi^{-1}(y)}{\partial y}\right|$$

## 3.4 Flow作为生成模型

### 基本定义
- **Flow**：函数 $\psi_t: \mathbb{R}^d \to \mathbb{R}^d$，将时间 $t$ 和位置 $x$ 映射到新位置
- **Flow模型**：$X_t = \psi_t(X_0)$，描述粒子的运动轨迹

### 马尔可夫性质
**证明**：

$$X_s = \psi_s(X_0) = \psi_s(\psi_t^{-1}(\psi_t(X_0))) = \psi_{s|t}(X_t)$$

其中 $\psi_{s\mid t} = \psi_s \circ \psi_t^{-1}$ 表示从时刻 $t$ 到 $s$ 的流映射。

**结论**：计算未来位置 $X_s$ 只需要当前位置 $X_t$，无需知道初始位置 $X_0$。

**Flow模型总结**：

$$X_1 = \psi_1(X_0) \sim q$$

### 3.4.1 Flow与速度场的等价性

直接定义复杂的变换 $\psi_t$ 困难，我们转而定义**瞬时速度** $u_t(x)$。

**常微分方程**：

$$\frac{d}{dt} \psi_t(x) = u_t(\psi_t(x)), \quad \psi_0(x) = x$$

这确保了flow的存在唯一性。

### 3.4.2 从源样本计算目标样本

已知速度场后，通过**数值求解ODE**生成目标样本：

**欧拉法**：

$$X_{t+h} = X_t + h \cdot u_t(X_t)$$

## 3.5 概率路径与连续性方程

### 概率路径
**定义**：$(p_t)_{0 \leq t \leq 1}$ 表示从初始密度 $p_0$ 到目标密度 $p_1$ 的连续演化过程。

**联系**：$X_t \sim p_t$ 将微观粒子运动与宏观分布演化联系起来。

### 生成定理
$u_t$ **生成** $p_t$ 当且仅当 $X_t = \psi_t(X_0) \sim p_t$ 对所有 $t$ 成立。

### 连续性方程

$$\frac{\partial p_t(x)}{\partial t} + \nabla \cdot (p_t(x)u_t(x)) = 0$$

**物理意义**：概率质量守恒定律。

**散度定理**：

$$\int\int\int_D (\nabla \cdot u) dV = \oint\oint_{\partial D} \langle u, n \rangle dS$$

## 3.6 瞬时变量变换

这一节讨论如何高效计算精确的对数似然。

### 核心公式

$$\frac{d}{dt} \log p_t(\psi_t(x)) = -\nabla \cdot u_t(\psi_t(x))$$

**积分形式**：

$$\log p_1(\psi_1(x)) = \log p_0(\psi_0(x)) - \int_0^1 \nabla \cdot u_t(\psi_t(x)) dt$$

### 哈钦森迹估计

$$\text{tr}(A) = E_Z[Z^T A Z]$$

其中 $Z$ 是标准高斯随机向量。

**应用**：

$$\nabla \cdot u_t(x) = \text{tr}[\nabla_x u_t(x)] = E_Z[\text{tr}[Z^T \nabla_x u_t(x) Z]]$$

因此：

$$\log p_1(\psi_1(x)) = \log p_0(\psi_0(x)) - E_Z\left[ \int_0^1 \text{tr}[Z^T \nabla_x u_t(\psi_t(x)) Z] dt \right]$$

## 3.7 传统Flow模型的训练复杂性

传统flow模型通过最小化KL散度训练，需要计算复杂的雅可比行列式，计算成本极高。这正是flow matching方法产生的动机。

---

# 第四章 Flow Matching的深入理论

## 4.1 核心问题陈述

**Flow Matching问题**：
寻找速度场 $u_t$ 生成概率路径 $p_t$，满足 $p_0 = p$ 且 $p_1 = q$。

**训练目标**：

$$L_{FM}(\theta) = E_{t, X_t \sim p_t} [D(u_t(X_t), u_t^\theta(X_t))]$$

本质上是随机在路径上找一个点，比较"标准答案"的速度和模型预测的速度差异，通过梯度下降优化神经网络参数 $\theta$。

**核心挑战**：如何确定"标准答案" $u_t(X_t)$？

## 4.2 数据耦合与概率路径构建

### 耦合的概念
**数据耦合**描述源样本 $X_0$ 和目标样本 $X_1$ 的关系，实际上是一个**联合分布**：

$$(X_0, X_1) \sim \pi_{0,1}(X_0, X_1)$$

### 耦合类型

**独立耦合 (Independent Coupling)**：

$$\pi_{0,1}(X_0, X_1) = p(X_0)q(X_1)$$

**依赖耦合 (Dependent Coupling)**：用于特定任务，如：
- 图像超分辨率：低分辨率 → 高分辨率
- 图像着色：灰度图像 → 彩色图像

### 条件概率路径
为简化问题，我们针对特定目标 $x_1$ 设计路径 $p_{t|1}(x|x_1)$：
- **起点**：$p_{0\mid 1}(x\mid x_1) = \pi_{0\mid 1}(x\mid x_1)$，在独立耦合场景下等于源分布 $p(x)$
- **终点**：$p_{1\mid 1}(x\mid x_1) = \delta_{x_1}(x)$（狄拉克分布）

**边缘化公式**：

$$p_t(x) = \int p_{t|1}(x|x_1)q(x_1)dx_1$$

这个公式联系具体样本到整体路径。

## 4.3 从条件到边缘的理论

### 条件速度场
定义：$u_t(\cdot|x_1)$ 生成 $p_{t|1}(\cdot|x_1)$

### 边缘速度场

$$u_t(x) = \int u_t(x|x_1) p_{t|1}(x_1|x) dx_1 = E[u_t(X_t|X_1) | X_t = x]$$

我们知道 $p_{t\mid 1}(x\mid x_1) = \frac{p_{t\mid 1}(x_1\mid x) q(x_1)}{p_t(x)}$，所以这实际上是一种最优估计。

### 数学推导基础

**边缘化公式**：

$$p_t(x) = \int p_{t|Z}(x|z) p_Z(z) dz$$

**速度场关系**：

$$u_t(x) = \frac{\int u_t(x|z) p_{t|Z}(x|z) p_Z(z) dz}{p_t(x)} = E[u_t(X_t|Z) | X_t = x]$$

**连续性方程推导**：

$$\frac{\partial p_t(x)}{\partial t} + \nabla \cdot (p_t(x) u_t(x)) = 0$$

详细推导：

$$\frac{d(p_t(x))}{dt} = \int \frac{d(p_{t|Z}(x|z) p_Z(z))}{dt} dz$$

$$= -\int \nabla_x[u_t(x|z) p_{t|Z}(x|z)] p_Z(z) dz \text{ （使用微观连续性方程）}$$

$$= -\nabla_x \int u_t(x|z) p_{t|Z}(x|z) p_Z(z) dz = -\nabla_x[u_t(x) p_t(x)]$$

## 4.4 损失函数的等价性

### 核心定理

$$\nabla_\theta L_{FM}(\theta) = \nabla_\theta L_{CFM}(\theta)$$

其中：
- $L_{FM}(\theta) = E_{t, x_t}[D(u_t(x_t), u_\theta^t(x_t))]$
- $L_{CFM}(\theta) = E_{t, z, x_{t\mid z}}[D(u_t(x_t\mid z), u_\theta^t(x_t))]$

### 通用学习原理
如果想训练函数 $g^\theta(X)$ 拟合条件期望 $E[Y|X]$，可通过最小化：

$$E_{X,Y}[D(Y, g^\theta(X))]$$

## 4.5 条件生成的实现策略

### 三步骤流程
1. **构建条件概率路径** $p_{t\mid z}(x\mid z)$：描述在每个中间时刻 $t$，给定条件 $z$ 时数据的概率分布
2. **设计条件速度场** $u_t(x\mid z)$
3. **使用条件流匹配损失训练**

### 条件流的具体形式

$$X_{t|1} = \psi_t(X_0|X_1)$$

其中 $X_0 \sim \pi_{0\mid 1}(\cdot\mid x_1)$（通常为高斯分布）

这个流模型 $\psi_t$ 输入起点 $x$ 和条件 $x_1$，输出时间 $t$ 时该点应在的位置。

### 边界条件

$$\psi_0(x|x_1) = x, \quad \psi_1(x|x_1) = x_1$$

即：刚出发时在起点，旅行结束时在终点。

**最简单实现**：

$$\psi_t(x|x_1) = (1-t) \cdot x + t \cdot x_1$$

### 速度场计算

$$u_t(x|x_1) = \dot{\psi}_t(\psi_t^{-1}(x|x_1)|x_1)$$

### 路径构建方式

1. **固定终点，起点随机**：$\psi_t(X_0, x_1)$
2. **固定起点，终点随机**：$\psi_t(x_0, X_1)$
3. **起点终点都随机**：$\psi_t(X_0, X_1)$

**通用插值器**：

$$\psi_t(x_0, x_1) = \alpha_t x_0 + \beta_t x_1$$

## 4.6 最优路径的选择

### 最优传输方法
**优化目标**：

$$\min \int_0^1 \int |u_t(x)|^2 p_t(x) dx dt$$

**约束条件**：
- 给定 $p_0, p_1$
- 连续性方程：$\frac{\partial p_t}{\partial t} + \nabla \cdot (p_t u_t) = 0$

**最优解**：

$$\psi_t^*(x) = t\phi(x) + (1-t)x$$

其中 $\phi$ 是**最优传输映射 (Optimal Transport map)**，能直接告诉源分布 $p_0$ 中的点 $x$ 在最优搬运方案下应该被搬到目标分布 $p_1$ 中的哪个点 $\phi(x)$。

### 简化方法
通过优化上界得到：

$$\psi_t(x|x_1) = t \cdot x_1 + (1-t) \cdot x$$

## 4.7 仿射条件流

### 通用形式

$$\psi_t(x|x_1) = \alpha_t \cdot x_1 + \sigma_t \cdot x$$

**边界条件**：
- $\alpha_0 = 0, \sigma_0 = 1$
- $\alpha_1 = 1, \sigma_1 = 0$

**单向性约束**：

$$\dot{\alpha}_t - \dot{\sigma}_t > 0$$

保证单向流动。

**速度场**：

$$u_t(x) = E[\dot{\alpha}_t \cdot X_1 + \dot{\sigma}_t \cdot X_0 | X_t = x]$$

## 4.8 预测策略

直接学习速度场较困难，可转而预测起点或终点。

### $x_1$-预测
神经网络 $\hat{x}_{1|t}(x)$ 的任务：给定路径中间点 $x$（时间 $t$ 产生），预测路径的目标终点 $x_1$。

$$\hat{x}_{1|t}(x) = E[X_1 | X_t = x]$$

### $x_0$-预测（去噪）
神经网络 $\hat{x}_{0|t}(x)$ 的任务：给定中间点 $x$，预测路径的起点 $x_0$。

$$\hat{x}_{0|t}(x) = E[X_0 | X_t = x]$$

在扩散模型中，这通常称为**去噪**，因为 $x_0$ 是清晰数据，$x$ 是加了噪声的数据。

### 速度场重构
因为有 $X_t = \alpha_t \cdot X_1 + \sigma_t \cdot X_0$，所以：

$$u_t(X_t) = \frac{d(X_t)}{dt} = \dot{\alpha}_t \cdot X_1 + \dot{\sigma}_t \cdot X_0$$

当我们只有 $X_t = x$ 时：

$$u_t(x) = \dot{\alpha}_t \cdot E[X_1 | X_t = x] + \dot{\sigma}_t \cdot E[X_0 | X_t = x]$$

简化记号：
- $x_{1\mid  t}(x) = E[X_1 \mid  X_t = x]$
- $x_{0\mid  t}(x) = E[X_0 \mid  X_t = x]$

可以进一步简化，仅保留 $E[X_0 \mid  X_t = x]$ 或 $E[X_1 \mid  X_t = x]$ 之一。

### 通用学习目标

$$g_t(x) := E[f_t(X_0, X_1) | X_t = x]$$

**损失函数**：

$$L_M(\theta) = E[D(g_t(X_t), g_t^\theta(X_t))]$$

## 4.9 调度器迁移理论

### 问题陈述
能否用调度器A训练的模型在推理时使用调度器B？

**答案**：可以，无需重新训练。

### 调度器定义
调度器 $(\alpha_t, \sigma_t)$ 定义数据在时刻 $t$ 的形式：

$$x_t = \alpha_t \cdot x_0 + \sigma_t \cdot z$$

- $\alpha_t$：**信号尺度因子**，控制原始清晰图像保留多少
- $\sigma_t$：**噪声尺度因子**，控制加入多少噪声

### 迁移条件
两个调度器等价当且仅当它们的**信噪比**相同：

$$\rho(t) = \frac{\alpha_t}{\sigma_t}$$

核心假设：如果新旧两个调度器在某时刻的信噪比相同，那么它们在模型看来就处于等效状态。

### 迁移公式推导

**假设**：

$$\psi_r(x_0|x_1) = s_r \cdot \psi_{t_r}(x_0|x_1)$$

**时间映射**：
为求出 $t_r$，需要对 $\rho(t)$ 求反函数：

$$t_r = \rho^{-1}(\rho(r))$$

**缩放因子**：
通过匹配 $\alpha_r x_1 + \sigma_r x_0 = s_r \cdot (\alpha_{t_r} x_1 + \sigma_{t_r} x_0)$ 中 $x_0$ 的系数：

$$\sigma_r = s_r \cdot \sigma_{t_r}$$

$$s_r = \frac{\sigma_r}{\sigma_{t_r}}$$

**速度场转换**：
已有旧调度器下的速度场 $u_t(x)$，需计算新调度器下的速度场 $u_r(x)$：


$$u_r(x) = E[\dot{X}_r | X_r = x]$$

$$= E[\dot{s}_r \cdot X_{t_r} + s_r \cdot \dot{X}_{t_r} \cdot \dot{t}_r | X_r = x]$$

$$= E[\dot{s}_r \cdot X_{t_r} | X_r = x] + E[s_r \cdot \dot{X}_{t_r} \cdot \dot{t}_r | X_r = x]$$

其中：
- $$
  E\!\left[\dot{s}_r\, X_{t_r} \,\middle|\, X_r = x\right]
  = \dot{s}_r\, E\!\left[X_{t_r} \,\middle|\, X_r = x\right]
  = \dot{s}_r \frac{x}{s_r}

  $$

- $$
  E\!\left[s_r\, \dot{X}_{t_r}\, \dot{t}_r \,\middle|\, X_r = x\right]
  = s_r\, \dot{t}_r\, E\!\left[\dot{X}_{t_r} \,\middle|\, X_{t_r} = \frac{x}{s_r}\right]
  = s_r\, \dot{t}_r\, u_{t_r}\!\left(\frac{x}{s_r}\right)

  $$

**最终公式**：

$$u_r(x) = \frac{\dot{s}_r}{s_r} \cdot x + s_r \cdot \dot{t}_r \cdot u_{t_r}\left(\frac{x}{s_r}\right)$$

### 详细推导过程

**步骤1：候选解构造**
基于旧系统流 $\psi$ 构造候选函数：

$$\phi(r, x_0) = s(r) \cdot \psi(t(r), x_0)$$

**步骤2：验证ODE满足**
目标：证明 $\frac{\partial \phi}{\partial r} = \bar{u}_r(\phi(r, x_0))$

**左侧**：

$$\text{LHS} = \frac{\partial}{\partial r}[s(r) \cdot \psi(t(r), x_0)]$$

$$= \dot{s}(r) \cdot \psi(t(r), x_0) + s(r) \cdot \frac{\partial \psi(t(r), x_0)}{\partial r}$$

$$= \dot{s}(r) \cdot \psi(t(r), x_0) + s(r) \cdot \frac{\partial \psi(t, x_0)}{\partial t}\bigg|_{t=t(r)} \cdot \dot{t}(r)$$

$$= \dot{s}(r) \cdot \psi(t(r), x_0) + s(r) \cdot u_{t(r)}(\psi(t(r), x_0)) \cdot \dot{t}(r)$$

**右侧**：

$$\text{RHS} = \bar{u}_r(\phi(r, x_0)) = \bar{u}_r(s(r) \cdot \psi(t(r), x_0))$$

使用 $$\bar{u}_r(x) = \frac{\dot{s}_r}{s_r}x + s_r\dot{t}_r u_{t_r}(\frac{x}{s_r})$$：

$$= \frac{\dot{s}(r)}{s(r)} \cdot [s(r)\psi(t(r), x_0)] + s(r)\dot{t}(r) u_{t(r)}\left(\frac{s(r)\psi(t(r), x_0)}{s(r)}\right)$$

$$= \dot{s}(r)\psi(t(r), x_0) + s(r)\dot{t}(r) u_{t(r)}(\psi(t(r), x_0))$$

**结论**：LHS = RHS，证毕。

### 等价性理论基础
无论使用哪个"合规"调度器，从同一初始噪声 $x_0$ 出发，最终在 $t=1$ 时刻生成的样本完全相同：

$$\bar{\psi}_1(x_0) = \psi_1(x_0)$$

## 4.10 高斯路径与调度器

### 高斯条件分布

$$p_{t|1}(x_t|x_1) = \mathcal{N}(\alpha_t x_1, \sigma_t^2 I)$$

### 调度器类型

**VP (Variance Preserving, 方差保持)**：
在加噪过程中，数据 $x_t$ 的整体方差大致保持不变。

$$\alpha_t^2 + \sigma_t^2 = 1$$

边界条件：
- $\alpha_1 = 1, \sigma_1 = 0$
- $\alpha_0 = 0, \sigma_0 = 1$

**VE (Variance Exploding, 方差爆炸)**：
数据 $x_t$ 的方差随加噪（时间从1到0）急剧增大。

$$\alpha_t = e^{-\frac{1}{2}\beta_t}, \quad \sigma_t = \sqrt{1 - e^{-\beta_t}}$$

### 得分函数与预测的等价性

**得分函数**：

$$\nabla \log p_t(x_t) = E[\nabla \log p_{t|1}(x_t|X_1)]$$

**高斯情况下**：

$$\nabla \log p_{t|1}(x_t|X_1) = -\frac{1}{\sigma_t^2}(x_t - \alpha_t X_1)$$

**边缘得分**：

$$\nabla \log p_t(x_t) = -\frac{1}{\sigma_t^2}(x_t - \alpha_t E[X_1|x_t])$$

实际上，预测"给定当前噪声图 $x_t$，对原始清晰图 $X_1$ 的最佳猜测（期望值）"就足够了！

### 等价预测方法
以下四种预测方式本质等价：
1. **$x_0$-预测**（或$x_1$-预测）：直接预测原始清晰图像
2. **得分预测**：直接预测得分（指南针方向）
3. **速度预测**：与"概率流ODE"生成方法相关
4. **噪声预测**（或$\varepsilon$-预测）：预测添加到 $x_0$ 上的噪声

## 4.11 数据耦合策略

### 独立耦合

$$\pi_{0,1}(x_0, x_1) = p(x_0)q(x_1)$$

适用于一般生成任务。

### 配对数据耦合

$$\pi_{0,1}(x_0, x_1) = \pi_{0|1}(x_0|x_1)q(x_1)$$

这是一一对应的关系，适用于：
- **图像修复**：完整图像 ↔ 部分遮挡图像
- **图像超分辨率**：高分辨率 ↔ 低分辨率
- **风格迁移**：目标风格 ↔ 原始图像

### 多样本耦合
$X_0$ 和 $X_1$ 没有天然配对关系时，可在每批次小样本数据内进行局部最优匹配。

**算法流程**：
1. **独立采样**：从 $p$ 和 $q$ 中分别采样 $k$ 个样本

   $$X_0^{(i)} \sim p, \quad X_1^{(i)} \sim q, \quad i \in [k]$$

2. **最优匹配**：构建最优匹配矩阵

   $$\pi^k = \arg\min_{\pi \in B_k} \sum_{i,j} \pi_{ij}^k c(X_0^{(i)}, X_1^{(j)})$$
   
   其中：
   - $B_k$：双随机矩阵集合（每行每列和为1）
   - $c(X_0^{(i)}, X_1^{(j)})$：成本函数（如欧氏距离平方）

3. **采样训练对**：根据匹配结果 $\pi^k$ 采样 $(X_0^{(i)}, X_1^{(j)})$ 训练对

**直观理解**：类似分配问题，将8只猫（工人）最优分配给8只狗（任务），使总成本最小。

## 4.12 条件生成与引导

### 4.12.1 条件模型
学习映射：$p(x) \to q(x|y)$

速度场变为：$u_t(x, y)$，同时依赖位置、时间和条件。

### 4.12.2 引导方法

**引导公式基础**：

$$\nabla\log p_{t|y}(x|y) = \nabla\log p_t(x) + \nabla\log p_{y|t}(y|x)$$

**有引导的得分 = 无引导的得分 + 分类器的得分**

**分类器引导 (Classifier Guidance)**：
分别训练生成模型（提供"无引导得分"）和分类器（提供"分类器得分"），生成时将两者相加实现引导。

**无分类器引导 (Classifier-free Guidance, CFG)**：
目前最流行的方法，不需要独立的分类器：

$$u_t^\theta(x|y) = (1-w)u_t^\theta(x|\emptyset) + w u_t^\theta(x|y)$$

其中：
- $w$：引导权重
- $\emptyset$：表示无条件
- 当 $w > 1$ 时，增强条件引导效果

---

# 总结与展望

本文系统地介绍了flow matching的完整理论框架，从基础的概率论概念出发，逐步构建了flow matching的数学基础，详细推导了关键公式，并探讨了实际应用中的技术细节。

## 核心要点回顾

1. **基本思想**：通过学习速度场引导粒子从简单分布流向复杂的目标分布
2. **关键创新**：使用条件流匹配损失避免了复杂的边缘速度场计算
3. **理论保证**：条件损失与边缘损失的梯度等价性
4. **实用技巧**：调度器迁移、多种预测策略、数据耦合方法
5. **应用拓展**：条件生成、引导采样、最优传输

## 技术优势

相比传统的flow模型和diffusion模型，flow matching具有以下优势：
- **训练简单**：避免了复杂的雅可比行列式计算
- **理论清晰**：数学推导严谨，物理意义明确
- **灵活性强**：支持多种路径设计和调度器
- **可扩展性**：易于扩展到条件生成和多模态任务

## 核心公式总结

**线性插值路径**：

$$X_t = (1-t)X_0 + tX_1$$

**条件速度场**：

$$u_t(x|x_1) = \frac{x_1 - x}{1-t}$$

**条件流匹配损失**：

$$L_{CFM}(\theta) = E_{t, X_0, X_1} \|u_t^\theta(X_t) - (X_1 - X_0)\|^2$$

**调度器迁移公式**：

$$u_r(x) = \frac{\dot{s}_r}{s_r} \cdot x + s_r \cdot \dot{t}_r \cdot u_{t_r}\left(\frac{x}{s_r}\right)$$

