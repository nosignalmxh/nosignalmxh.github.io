---
title: 'flow matching × Single cell Dynamics(1): Improving and Generalizing Flow-Based Generative Models  with Minibatch Optimal Transport'
date: 2025-10-07
permalink: /posts/2025/10/blog-post-15/
tags:
  - 论文
  - 机器学习
---

接下来开一个新系列，记录自己读的有关flow matching和single cell dynamics交叉领域的paper。第一节记录一下这个领域最经典的paper：Improving and Generalizing Flow-Based Generative Models  with Minibatch Optimal Transport，从最经典朴素的CFM,OTCFM,SBCFM开始讲起。

# 1.背景介绍

Alexander Tong这篇文章，关注用连续时间流模型连接源分布与目标分布，并在不需要“模拟时间动态”进行训练的前提下，统一并推广现有的 conditional flow matching 思路。

## 1.1从基于模拟到免模拟的范式转变

传统的生成模型，如早期归一化流（Normalizing Flows），通过组合静态可逆模块来构建分布间的映射。后续的连续归一化流（Continuous Normalizing Flows, CNF）则利用神经网络驱动的常微分方程（Neural ODEs）来实现这一过程，虽然理论上更灵活，但其训练依赖于数值积分和对求解器进行反向传播，这带来了巨大的计算开销和数值不稳定性，限制了其扩展性。

与此同时，扩散模型（Diffusion Models）作为当前最先进的生成技术，其成功部分归功于一个简单且稳定的训练目标：直接对随机微分方程（SDE）的漂移项或得分函数进行回归，而无需在训练时模拟整个轨迹。这一思想启发了针对CNF的改进：能否也为其设计一个类似的“免模拟”训练框架呢？这正是近期流匹配（Flow Matching, FM）工作的出发点，但早期的FM方法通常假设源分布必须为高斯分布，限制了其通用性。

此外，无论是经过改进的CNF还是扩散模型，都面临一个共同的瓶颈：推理（采样）速度慢，因为生成一个高质量样本需要对ODE/SDE进行多步数值求解。

# 2.核心背景知识

本文提出的条件流匹配（CFM）框架，是在连续归一化流（CNF）、流匹配（FM）以及最优传输（OT）这三大理论支柱上构建的。本节将详细阐述这些基础概念，并剖析它们如何为本文的核心贡献铺平道路。

## 2.1 概率流、神经ODE与流匹配（FM）

本文的核心任务是学习一个映射 $f: \mathbb{R}^d \to \mathbb{R}^d$，它能够将一个源概率分布 $q_0$ 精确地变换为目标概率分布 $q_1$。即，若 $x_0 \sim q_0(x_0)$，则 $f(x_0) \sim q_1(x_1)$。这一设定非常通用，既涵盖了从简单先验（如高斯分布）生成复杂数据（如图像）的传统任务，也适用于源和目标均为经验数据分布的场景（如单细胞轨迹推断）。

### 2.1.1 模型基础：作为生成器的神经ODE (Neural ODEs)

连续归一化流（CNF）通过一个由神经网络参数化的**常微分方程（ODE）**来实现这一映射：
$$
dx = u_t(x)\,dt \quad (1)
$$
这里的 $u_t(x)$ 是一个时间依赖的**向量场 (vector field)**，它为时空中的任意一点 $(t, x)$ 指定了一个“速度”或“运动方向”。通过从 $t=0$ 到 $t=1$ 对这个ODE进行积分，我们可以得到其解 $\phi_t(x)$，这是一个从初始点 $x_0$ 到 $t$ 时刻位置 $x_t$ 的连续变换。

当我们将这个变换应用于整个分布 $p_0$ 时，该分布会沿着由 $u_t$ 定义的流线进行演化，这个过程被称为**概率流 (probability flow)**。其在 $t$ 时刻的分布 $p_t$ 是初始分布 $p_0$ 经过映射 $\phi_t$ 的**前推测度 (pushforward measure)**，记为 $p_t = [\phi_t]_{\#} p_0$。这一动态过程由物理学中著名的**连续性方程**所描述：
$$
\frac{\partial p_t}{\partial t} = -\nabla\cdot\big(p_t(x)\,u_t(x)\big) \quad (2)
$$
这个方程建立了概率密度 $p_t$ 的时间演化与驱动它的向量场 $u_t$ 之间的直接联系。

### 2.1.2 训练范式：流匹配 (Flow Matching, FM)

为了让模型学习到理想的向量场，Lipman et al. (2023) 提出了**流匹配 (Flow Matching, FM)** 目标。其核心思想是，如果我们预先知道了一条连接 $q_0$ 和 $q_1$ 的理想路径 $\{p_t, u_t\}$，我们就可以通过一个简单的**回归损失**来训练神经网络 $v_\theta(t,x)$ 去逼近真实的向量场 $u_t(x)$：
$$
\mathcal{L}_{\rm FM}(\theta)=\mathbb{E}_{t\sim\mathcal{U}(0,1),\,x\sim p_t(x)}\,\|v_\theta(t,x)-u_t(x)\|^2 \quad (3)
$$
然而，这个目标在一般情况下是**难以计算的 (intractable)**，因为对于任意的 $q_0$ 和 $q_1$，其中间时刻的概率密度 $p_t$ 和向量场 $u_t$ 通常没有解析表达式。这正是本文CFM框架试图解决的核心挑战。

### 2.1.3 关键构建模块：高斯路径的特例

解决上述难题的突破口在于一个特殊的、可解的情形：当概率流 $p_t$ 是一条连接两个高斯分布的路径，即 $p_t=\mathcal{N}(\mu_t,\sigma_t^2 I)$ 时，其对应的向量场 $u_t(x)$ 具有一个已知的**闭式解 (closed-form solution)**：
$$
u_t(x)=\frac{\sigma_t'}{\sigma_t}\big(x-\mu_t\big)+\mu_t' \quad (5)
$$
其中 $\mu_t'$ 和 $\sigma_t'$ 分别是均值和标准差路径关于时间 $t$ 的导数。这个结论在推导后续CFM的内容中会有重要的作用。

## 2.2 静态与动态最优传输 (OT)

在能够连接两个分布的基础上，我们进一步追求一条“最优”的路径。最优传输（Optimal Transport, OT）理论为此提供了严谨的数学框架。

### 2.2.1 静态最优传输：寻找最优“耦合”

静态OT（以Kantorovich形式为例）旨在找到一种成本最低的“运输方案”，将分布 $q_0$ 的质量搬运到 $q_1$。在欧氏距离代价下，**2-Wasserstein距离** 的平方定义如下：
$$
W_2^2(q_0,q_1)=\inf_{\pi\in\Pi(q_0,q_1)}\int_{\mathbb{R}^d\times\mathbb{R}^d}\|x-y\|^2\,d\pi(x,y) \quad (6)
$$
其中 $\Pi(q_0,q_1)$ 是所有以 $q_0$ 和 $q_1$ 为**边缘分布 (marginals)** 的联合分布（也称**耦合, coupling**）的集合。这个公式寻找的是一个最优耦合 $\pi^*$，它使得配对点 $(x,y)$ 之间距离平方的期望最小。

### 2.2.2 动态最优传输：寻找能量最低的“路径”

动态OT（以Benamou–Brenier公式为例）从流体动力学的视角刻画了同一问题。它将 $W_2^2$ 定义为一个在所有可能的概率流路径 $\{p_t, u_t\}$ 上的变分问题，目标是最小化路径的总**动能 (kinetic energy)**：
$$
W_2^2(q_0,q_1)=\inf_{\{p_t,u_t\}}\int_0^1\!\int_{\mathbb{R}^d} p_t(x)\,\|u_t(x)\|^2\,dx\,dt \quad (7)
$$
该优化受限于边界条件 $p_0=q_0, p_1=q_1$ 和连续性方程 (2)。在温和的正则性假设下，静态与动态形式被证明是等价的。

## 2.3 从边际流到条件流的等价变换

在为连续归一化流（CNF）构建一个免于模拟的训练目标时，核心挑战在于如何定义一个既正确又易于计算的回归目标。如果我们已经知道一个概率路径 $p_t(x)$ 及其对应的向量场（即漂移项）$u_t(x)$，那么可以直接使用 Flow Matching (FM) 损失 $\mathcal{L}_{FM}(\theta) = \mathbb{E}_{t,x\sim p_t(x)}\|v_\theta(t,x) - u_t(x)\|^2$ 来训练一个神经网络 $v_\theta$ 进行拟合。然而在多数情况下，这个理想的边际向量场 $u_t(x)$ 本身是未知的，或者形式极其复杂以至于无法直接计算。

### 2.3.1 核心思想：将复杂边际路径分解为简单条件路径的混合

作者首先提出了一个核心设定：任何我们希望建模的复杂（边际）概率路径 $p_t(x)$，都可以被视为由某个条件变量 $z$ 所引导的、一系列简单“条件”概率路径 $p_t(x|z)$ 的加权混合。其数学表达为：

$$p_t(x) = \int p_t(x|z)q(z)dz$$

其中 $q(z)$ 是条件变量的分布。那么在这个设定之下，就可以自由设计那些易于处理的条件路径 $p_t(x|z)$ 及其对应的、形式简单的条件向量场 $u_t(x|z)$来简化计算了。

### 2.3.2 混合路径对应的边际向量场

基于上述设定，论文通过定理 3.1 证明，生成该混合路径的边际向量场 $u_t(x)$ 具有如下形式：

$$u_t(x) := \mathbb{E}_{q(z)}\frac{u_t(x|z)p_t(x|z)}{p_t(x)}$$

虽然这样我们的到来理想的回归目标，但是在实践中，我们很难求解出母中的边际密度 $p_t(x)$，从而也就很难计算出 $u_t(x)$ 作为训练时的靶标。

### 2.3.3 转化训练目标

为了绕过这一障碍，CFM系列的工作使用的是另一种目标函数--**条件流匹配（Conditional Flow Matching, CFM）目标函数**：

$$\mathcal{L}_{CFM}(\theta) := \mathbb{E}_{t,q(z),p_t(x|z)}\|v_\theta(t,x) - u_t(x|z)\|^2$$

使用这个目标函数，就可以将神经网络 $v_\theta$ 的回归目标从那个无法计算的边际向量场 $u_t(x)$，替换为了我们**可以自由设计且易于计算的条件向量场** $u_t(x|z)$。

### 2.3.4 CFM的数学合法性

文中的**定理 3.2**为上述替换操作提供了数学证明：在 $p_t(x) > 0$ 的条件下，虽然 $\mathcal{L}_{CFM}$ 和 $\mathcal{L}_{FM}$ 的值不同，但它们**关于模型参数 $\theta$ 的梯度是完全相等的**：

$$\nabla_\theta \mathcal{L}_{CFM}(\theta) = \nabla_\theta \mathcal{L}_{FM}(\theta)$$

所以我们可以通过最小化一个简单、可计算的CFM损失，来实现与最小化那个复杂、不可计算的理想FM损失完全相同的优化效果。所以我们只需设计出合适的条件路径，便可利用CFM目标函数，让模型自动学习到正确的、复杂的边际动力学，而无需关心其背后难以名状的真实形式。

# 3. 方法：条件流匹配 (CFM) 的三种变体

论文给出了 Conditional Flow Matching（CFM）的三种重要实例：I‑CFM、OT‑CFM、SB‑CFM。这三种模型都是CFM家族的一员，区别在于下面三个不同点：

1.  **$q(z)$：配对策略**。如何从源数据分布$q_0$和目标数据分布$q_1$中抽取样本对$(x_0, x_1)$。
2.  **$p_t(x|z)$：条件路径**。对于已经配好的一对$(x_0, x_1)$，他们之间的变换路径(直线/随机抖动的“桥”)。
3.  **$u_t(x|z)$：路径速度向量**。在这条假定的路径上，任意一个点$(t, x)$的瞬时速度（方向和大小），作为提供给神经网络学习的“监督信号”。

## 3.1 I-CFM：最简单的“随机配对”策略

**I-CFM (Independent CFM)** 是最基础、最直接的实现方式。它的核心思想是：我们不需要任何花哨的技巧，就用最简单的“随机抽样”和“直线路径”来构建训练任务。

-   **配对策略 $q(z)$：独立耦合 (Independent Coupling)**
    -   **是什么**：$z=(x_0, x_1)$，其中$x_0 \sim q_0$和$x_1 \sim q_1$是**完全独立**地随机抽取的。即 $q(z)=q(x_0)q(x_1)$。
    -   **好比是**：在两个装满了球的桶里，完全随机地各摸出一个球，将它们配成一对。

-   **条件路径 $p_t(x|z)$ 与 速度向量 $u_t(x|z)$：高斯平滑的直线**
    -   **路径**：对于配好的一对$(x_0, x_1)$，我们假设变换路径就是一条从$x_0$到$x_1$的**直线**。为了让学习过程更平滑，我们给这条直线上的每个点都加上了一点固定的高斯噪声（方差为$\sigma^2$）。
        $$p_t(x\mid z)=\mathcal{N}\big(x\,\big|\,(1-t)\,x_0+t\,x_1,\;\sigma^2\mathbf{I}\big)$$
    -   **速度**：因为路径是匀速直线，所以任何时间、任何地点的速度向量都是恒定的，即从起点指向终点的向量 $(x_1-x_0)$。
        $$u_t(x\mid z) = x_1-x_0$$
    -   **训练信号**：这意味着神经网络的训练目标极其简单——无论输入的时间$t$和位置$x$是什么，都请输出固定的向量 $(x_1-x_0)$。

-   **最终效果与直观理解**
    -   **优点**：实现简单，计算开销小，非常稳健。它成功地将Flow Matching方法推广到了任意数据分布，不再局限于高斯噪声源。
    -   **缺点**：由于是随机配对，可能会出现一些“低效”的路径（比如把一个远在天边的样本配对过来），这导致整体的训练信号**方差较大**。最终学到的全局流（Flow）虽然是正确的，但可能比较“弯曲”，导致在**推理生成时需要更多的计算步数（NFE）**才能得到好的结果。

---

## 3.2 OT-CFM：更高效的“最优匹配”策略

**OT-CFM (Optimal Transport CFM)** 是对I-CFM的直接升级。它意识到“配对”环节至关重要，因此引入了最优传输（OT）来寻找最高效的配对方案，旨在学习到最“直”的全局流。

-   **配对策略 $q(z)$：最优传输耦合 (Optimal Transport Coupling)**
    -   **是什么**：$z=(x_0, x_1)$不再独立抽取，而是根据一个**最优传输计划** $\pi(x_0, x_1)$ 来联合抽取。这个计划能保证整个数据分布变换的“总路程”或“总代价”最小。
    -   **好比是**：我们不再随机摸球，而是请一位仓储优化大师（OT算法）来规划，哪个桶里的哪个球应该送到另一个桶的哪个位置，才能让所有球的平均搬运距离最短。
    -   **实践中的技巧**：对于大数据集，计算全局OT计划不现实。因此论文采用**小批量最优传输 (Minibatch OT)**：在每个训练批次内部，快速求解一次OT问题来获得近似的“最优配对”。

-   **条件路径 $p_t(x|z)$ 与 速度向量 $u_t(x|z)$：依然是高斯平滑的直线**
    -   这部分与I-CFM**完全相同** 。对于已经“最优匹配”好的一对$(x_0, x_1)$，我们依然假设它们走的是直线路径，速度向量也依然是 $(x_1-x_0)$。
    -   **关键区别**：OT-CFM的巧妙之处在于，它通过优化**配对环节**，使得提供给模型的这些“直线路径”本身就更有意义、更“直”，从而让最终学到的全局流自然地趋向于最优。

-   **最终效果与直观理解**
    -   **优点**：
        1.  **训练更高效**：由于配对更合理，训练信号的**方差显著降低**，模型收敛更快、更稳定。
        2.  **推理更快速**：学到的流路径非常接近理论上的“最短路径”（动态OT），因此在生成样本时可以用**更少的计算步数（NFE）**达到同样甚至更好的质量。
    -   **开销**：训练时只比I-CFM增加了一点点计算批内OT的开销，但换来的收益是巨大的。

---

## 3.3 SB-CFM：连接确定性与随机性的“布朗桥”策略

**SB-CFM (Schrödinger Bridge CFM)** 可以看作是OT-CFM的“带噪声”或“熵正则化”版本。它不仅是一个强大的生成模型，还为解决物理学和概率论中的“薛定谔桥”问题提供了一个全新的、无需模拟的求解器。

-   **配对策略 $q(z)$：熵正则化最优传输耦合**
    -   **是什么**：它使用的配对策略 $\pi_{2\sigma^2}(x_0, x_1)$ 是在最优传输的基础上增加了一项“熵正则项”。
    -   **好比是**：仓储优化大师在规划路径时，不再只追求绝对的最短距离，还会稍微增加一点随机性（比如允许一些不是最优但也很不错的路径），使得整个搬运计划更加鲁棒和灵活。这个随机性的强度由$\sigma$控制。

-   **条件路径 $p_t(x|z)$ 与 速度向量 $u_t(x|z)$：布朗桥 (Brownian Bridge)**
    -   **路径**：它不再假设路径是笔直的，而是假设路径是一个**布朗桥** [cite: 243]。
        $$p_t(x\mid z)=\mathcal{N}\big(x\,\big|\,(1-t)x_0+tx_1,\;t(1-t)\,\sigma^2\mathbf{I}\big)$$
        -   **直观理解**：想象一根两端固定在$x_0$和$x_1$的琴弦，它在中间会随机振动。这个路径的噪声方差是随时间变化的，在起点和终点($t=0, 1$)为0，在中间($t=0.5$)最大。
    -   **速度**：由于路径是带随机性的曲线，其速度向量也变得更复杂。
        $$u_t(x\mid z)=\frac{1-2t}{2t(1-t)}\Big(x-\big((1-t)x_0+tx_1\big)\Big)+(x_1-x_0)$$
        -   它包含两部分：一个是指向终点的**基础速度** $(x_1-x_0)$，另一个是**修正项**，如果当前位置$x$偏离了路径的中心线，这个修正项会把它“拉回来”。

-   **最终效果与直观理解**
    -   **关键作用**：SB-CFM学习的是薛定谔桥问题的确定性部分（即概率流ODE）。它在理论上非常优美，将I-CFM和OT-CFM统一了起来：
        -   当噪声$\sigma \to 0$时，熵正则OT退化为标准OT，布朗桥退化为直线，**SB-CFM → OT-CFM** [cite: 251]。
        -   当噪声$\sigma \to \infty$时，熵正则OT的配对趋向于独立耦合，**SB-CFM → I-CFM** [cite: 251]。
    -   这提供了一个通过调节超参数$\sigma$在**效率（OT-CFM）**和**随机性/鲁棒性**之间进行权衡的强大工具。